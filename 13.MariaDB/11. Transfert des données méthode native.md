Voici l’article complet, restructuré en **construction progressive**, avec **blocs courts**, **fichiers sources fournis (CSV/JSON/XML)**, et une **explication détaillée de l’utilisation des scripts** dans `mysql` et MySQL Workbench.

---

# 1. Contexte et objectifs de l’atelier

Dans cet atelier, on exploite **la méthode native MariaDB** (SQL pur + procédures stockées + événements) pour réaliser des transferts de données et de petits ETL :

- **Dans la même instance MariaDB** (base source → base cible).

- **Entre deux instances MariaDB** (serveur A → serveur B).

- **Avec des fichiers externes** : CSV, JSON, XML.

- **Avec automatisation** : procédures stockées, événements, scripts externes.

L’objectif est de partir de scripts **très simples** puis de les **faire évoluer par petites étapes**, sans noyer l’étudiant dans des pavés de 100 lignes.

---

# 2. Préparation de l’environnement MariaDB

On part sur une instance MariaDB avec trois schémas :

- `source_db` : données brut / opérationnelles.

- `target_db` : données nettoyées ou consolidées.

- `etl` : tables de staging, logs, procédures ETL.

## 2.1 Création des bases et de l’utilisateur ETL

Dans `mysql` ou MySQL Workbench, exécutez :

```sql
-- Création des bases de travail
CREATE DATABASE IF NOT EXISTS source_db;
CREATE DATABASE IF NOT EXISTS target_db;
CREATE DATABASE IF NOT EXISTS etl;

-- Création d'un utilisateur dédié aux opérations ETL
CREATE USER IF NOT EXISTS 'etl_user'@'%' IDENTIFIED BY 'Mot2Passe!';

-- Droits suffisants pour l'atelier
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP,
      CREATE TEMPORARY TABLES, EXECUTE, FILE
ON *.* TO 'etl_user'@'%';

FLUSH PRIVILEGES;
```

---

## 2.2 Création d’une table source de démonstration

Toujours dans MariaDB :

```sql
USE source_db;

-- Table source : clients, version simplifiée pour la démo
CREATE TABLE IF NOT EXISTS clients_src (
  id INT AUTO_INCREMENT PRIMARY KEY,
  nom VARCHAR(100),
  email VARCHAR(200),
  date_naissance DATE,
  date_creation DATETIME DEFAULT CURRENT_TIMESTAMP,
  total_achats DECIMAL(10,2) DEFAULT 0
);

-- Quelques données d'exemple
INSERT INTO clients_src (nom, email, date_naissance, total_achats)
VALUES
  ('alice', '  alice@example.com ', '1990-05-10', 1500),
  ('bob',   'bob(at)example.com',   '1985-03-21', 200),
  ('CAROL', 'carol@example.com',    '1978-11-02', 12000);
```

## 2.3 Table cible de démonstration

```sql
USE target_db;

-- Table cible : version "propre" de la table clients
CREATE TABLE IF NOT EXISTS clients_dest (
  id INT AUTO_INCREMENT PRIMARY KEY,
  nom_normalise VARCHAR(100),
  email_valide VARCHAR(200),
  age INT,
  segment VARCHAR(20),
  date_integration DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

---

# 3. Transfert intra-instance : construire progressivement le flux SQL

On commence par **le même serveur** (intra-instance) : `source_db.clients_src` → `target_db.clients_dest`.

---

## 3.1 Étape 1 – Copier “bête et méchant” (sans transformation)

On se place dans `target_db` :

```sql
USE target_db;

-- Copie brute depuis la base source, sans transformation
INSERT INTO clients_dest (nom_normalise, email_valide, age, segment)
SELECT 
  nom,      -- on copie le nom tel quel
  email,    -- on copie l'email tel quel
  NULL,     -- age inconnu pour le moment
  NULL      -- segment inconnu pour le moment
FROM source_db.clients_src;
```

**Idée clé :**  
On commence avec un transfert **très simple**, juste pour appréhender `INSERT ... SELECT` entre bases du même serveur.

---

## 3.2 Étape 2 – Ajouter un filtre simple

On ne veut que les clients récents (créés depuis 30 jours) :

```sql
USE target_db;

INSERT INTO clients_dest (nom_normalise, email_valide, age, segment)
SELECT 
  nom,
  email,
  NULL,
  NULL
FROM source_db.clients_src
WHERE date_creation >= CURDATE() - INTERVAL 30 DAY;
```

**Progression :**  
On introduit la **condition `WHERE`** dans le transfert.

---

## 3.3 Étape 3 – Ajouter des transformations

On profite du transfert pour **nettoyer** les données.

```sql
USE target_db;

INSERT INTO clients_dest (nom_normalise, email_valide, age, segment)
SELECT 
  -- Nom : première lettre en majuscule, le reste en minuscule
  CONCAT(
    UPPER(SUBSTRING(nom, 1, 1)),
    LOWER(SUBSTRING(nom, 2))
  ) AS nom_normalise,

  -- Email : trim + validation simple via regex
  CASE 
    WHEN TRIM(email) REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'
    THEN TRIM(LOWER(email))
    ELSE NULL
  END AS email_valide,

  -- Age approximatif
  TIMESTAMPDIFF(YEAR, date_naissance, CURDATE()) AS age,

  -- Segmentation simple
  CASE 
    WHEN total_achats > 10000 THEN 'Premium'
    WHEN total_achats > 1000 THEN 'Standard'
    ELSE 'Basic'
  END AS segment
FROM source_db.clients_src;
```

**Message pour l’étudiant :**  
Le transfert n’est pas qu’une “copie” : c’est déjà une **vraie étape de transformation ETL**.

---

## 3.4 Étape 4 – Encapsuler dans une procédure (version minimale)

On transforme le bloc précédent en **procédure stockée** réutilisable.

```sql
USE target_db;

DELIMITER $$

CREATE PROCEDURE etl_import_clients_basic()
BEGIN
  -- On vide la table cible pour la démo (attention en prod)
  TRUNCATE TABLE clients_dest;

  -- Reprise du transfert avec transformation
  INSERT INTO clients_dest (nom_normalise, email_valide, age, segment)
  SELECT 
    CONCAT(
      UPPER(SUBSTRING(nom, 1, 1)),
      LOWER(SUBSTRING(nom, 2))
    ),
    CASE 
      WHEN TRIM(email) REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'
      THEN TRIM(LOWER(email))
      ELSE NULL
    END,
    TIMESTAMPDIFF(YEAR, date_naissance, CURDATE()),
    CASE 
      WHEN total_achats > 10000 THEN 'Premium'
      WHEN total_achats > 1000 THEN 'Standard'
      ELSE 'Basic'
    END
  FROM source_db.clients_src;
END$$

DELIMITER ;
```

Exécution :

```sql
CALL etl_import_clients_basic();
```

---

## 3.5 Étape 5 – Ajouter un log simple (version légèrement enrichie)

On crée une table de log ETL dans le schéma `etl`.

```sql
USE etl;

CREATE TABLE IF NOT EXISTS log_import_clients (
  id INT AUTO_INCREMENT PRIMARY KEY,
  date_execution DATETIME,
  lignes_inserées INT
);
```

Puis on enrichit la procédure :

```sql
USE target_db;

DELIMITER $$

CREATE PROCEDURE etl_import_clients()
BEGIN
  DECLARE v_lignes INT;

  TRUNCATE TABLE clients_dest;

  INSERT INTO clients_dest (nom_normalise, email_valide, age, segment)
  SELECT 
    CONCAT(
      UPPER(SUBSTRING(nom, 1, 1)),
      LOWER(SUBSTRING(nom, 2))
    ),
    CASE 
      WHEN TRIM(email) REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'
      THEN TRIM(LOWER(email))
      ELSE NULL
    END,
    TIMESTAMPDIFF(YEAR, date_naissance, CURDATE()),
    CASE 
      WHEN total_achats > 10000 THEN 'Premium'
      WHEN total_achats > 1000 THEN 'Standard'
      ELSE 'Basic'
    END
  FROM source_db.clients_src;

  -- Nombre de lignes transférées
  SET v_lignes = ROW_COUNT();

  -- Insertion du log dans la base etl
  INSERT INTO etl.log_import_clients (date_execution, lignes_inserées)
  VALUES (NOW(), v_lignes);
END$$

DELIMITER ;
```

Appel :

```sql
CALL etl_import_clients();
```

---

# 4. Transfert inter-instances : deux serveurs MariaDB

On passe maintenant à un scénario **serveur A → serveur B**.

- `serveur_a` : source, base `source_db`.

- `serveur_b` : cible, base `target_db`.

On présente deux méthodes progressives :

1. **Dump SQL complet (mysqldump)** – basique mais robuste.

2. **CSV + LOAD DATA** – mieux adapté aux très gros volumes.

---

## 4.1 Méthode 1 – Dump SQL (migration simple)

### 4.1.1 Export depuis le serveur A

Dans un terminal (Linux ou PowerShell) relié à `serveur_a` :

```bash
# Export de la table clients_src vers un fichier SQL
mysqldump -h serveur_a -u etl_user -p source_db clients_src > clients_src.sql
```

### 4.1.2 Transfert du fichier vers le serveur B

Exemple avec `scp` (Linux) :

```bash
scp clients_src.sql etl_user@serveur_b:/tmp/
```

### 4.1.3 Import dans la base cible sur le serveur B

Sur `serveur_b` :

```bash
mysql -h localhost -u etl_user -p target_db < /tmp/clients_src.sql
```

**Idée :**  
On montre la méthode la plus universelle, même si elle n’est pas la plus fine pour un vrai ETL.

---

## 4.2 Méthode 2 – CSV + LOAD DATA (flux plus efficaces)

### 4.2.1 Export CSV sur `serveur_a`

```sql
USE source_db;

SELECT id, nom, email, date_naissance, total_achats
INTO OUTFILE '/tmp/clients_export.csv'
FIELDS TERMINATED BY ','
ENCLOSED BY '"'
LINES TERMINATED BY '\n'
FROM clients_src;
```

### 4.2.2 Transfert du CSV vers `serveur_b`

```bash
scp /tmp/clients_export.csv etl_user@serveur_b:/tmp/
```

### 4.2.3 Import CSV sur `serveur_b` dans une table de staging

```sql
USE etl;

CREATE TABLE IF NOT EXISTS clients_staging (
  id INT,
  nom VARCHAR(100),
  email VARCHAR(200),
  date_naissance DATE,
  total_achats DECIMAL(10,2)
);

LOAD DATA INFILE '/tmp/clients_export.csv'
INTO TABLE clients_staging
FIELDS TERMINATED BY ','
ENCLOSED BY '"'
LINES TERMINATED BY '\n';
```

### 4.2.4 Transformation vers la table finale sur `serveur_b`

```sql
USE target_db;

INSERT INTO clients_dest (nom_normalise, email_valide, age, segment)
SELECT
  CONCAT(
    UPPER(SUBSTRING(nom, 1, 1)),
    LOWER(SUBSTRING(nom, 2))
  ),
  CASE 
    WHEN TRIM(email) REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'
    THEN TRIM(LOWER(email))
    ELSE NULL
  END,
  TIMESTAMPDIFF(YEAR, date_naissance, CURDATE()),
  CASE 
    WHEN total_achats > 10000 THEN 'Premium'
    WHEN total_achats > 1000 THEN 'Standard'
    ELSE 'Basic'
  END
FROM etl.clients_staging;
```

---

# 5. ETL avec fichiers externes : CSV, JSON, XML

On intègre maintenant de **vrais fichiers** externes pour rendre la démo concrète.

---

## 5.1 Cas CSV – Fichier clients

### 5.1.1 Créer un fichier CSV d’exemple

Créer le fichier **data/clients.csv** :

```bash
"nom","email","date_naissance","total_achats"
"alice","alice@example.com","1990-05-10","1500.00"
"bob","bob(at)example.com","1985-03-21","200.00"
"CAROL","carol@example.com","1978-11-02","12000.00"
```

### 5.1.2 Charger ce CSV dans une table de staging

```sql
USE etl;

CREATE TABLE IF NOT EXISTS clients_csv_staging (
  nom VARCHAR(100),
  email VARCHAR(200),
  date_naissance DATE,
  total_achats DECIMAL(10,2)
);

LOAD DATA INFILE '/path/vers/data/clients.csv'
INTO TABLE clients_csv_staging
FIELDS TERMINATED BY ','
ENCLOSED BY '"'
IGNORE 1 LINES;  -- on ignore l'entête
```

> Adapter `/path/vers/data` en fonction du chemin réel sur le serveur.

### 5.1.3 Transférer vers `target_db` avec transformation

```sql
USE target_db;

INSERT INTO clients_dest (nom_normalise, email_valide, age, segment)
SELECT
  CONCAT(
    UPPER(SUBSTRING(nom, 1, 1)),
    LOWER(SUBSTRING(nom, 2))
  ),
  CASE 
    WHEN TRIM(email) REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'
    THEN TRIM(LOWER(email))
    ELSE NULL
  END,
  TIMESTAMPDIFF(YEAR, date_naissance, CURDATE()),
  CASE 
    WHEN total_achats > 10000 THEN 'Premium'
    WHEN total_achats > 1000 THEN 'Standard'
    ELSE 'Basic'
  END
FROM etl.clients_csv_staging;
```

---

## 5.2 Cas JSON – Fichier clients.json

### 5.2.1 Créer le fichier JSON

Créer le fichier **data/clients.json** :

```bash
[
  {
    "nom": "alice",
    "email": "alice@example.com",
    "date_naissance": "1990-05-10",
    "total_achats": 1500.00
  },
  {
    "nom": "bob",
    "email": "bob(at)example.com",
    "date_naissance": "1985-03-21",
    "total_achats": 200.00
  },
  {
    "nom": "CAROL",
    "email": "carol@example.com",
    "date_naissance": "1978-11-02",
    "total_achats": 12000.00
  }
]
```

### 5.2.2 Table staging JSON

```sql
USE etl;

CREATE TABLE IF NOT EXISTS clients_json_raw (
  json_doc JSON
);
```

### 5.2.3 Charger le JSON brut

Si MariaDB a accès au fichier via le système :

```sql
LOAD DATA INFILE '/path/vers/data/clients.json'
INTO TABLE clients_json_raw;
```

Ou coller le contenu à la main :

```sql
INSERT INTO clients_json_raw (json_doc)
VALUES (LOAD_FILE('/path/vers/data/clients.json'));
```

### 5.2.4 Extraction vers la table finale (version simple)

On suppose que chaque document JSON est un tableau d’objets (`[ { ... }, { ... } ]`).

```sql
USE target_db;

INSERT INTO clients_dest (nom_normalise, email_valide, age, segment)
SELECT
  CONCAT(
    UPPER(SUBSTRING(j.nom, 1, 1)),
    LOWER(SUBSTRING(j.nom, 2))
  ),
  CASE 
    WHEN TRIM(j.email) REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'
    THEN TRIM(LOWER(j.email))
    ELSE NULL
  END,
  TIMESTAMPDIFF(YEAR, j.date_naissance, CURDATE()),
  CASE 
    WHEN j.total_achats > 12000 THEN 'Premium'
    WHEN j.total_achats > 1000 THEN 'Standard'
    ELSE 'Basic'
  END
FROM etl.clients_json_raw AS r,
JSON_TABLE(
  r.json_doc,
  '$[*]' COLUMNS (
    nom           VARCHAR(100) PATH '$.nom',
    email         VARCHAR(200) PATH '$.email',
    date_naissance DATE        PATH '$.date_naissance',
    total_achats  DECIMAL(10,2) PATH '$.total_achats'
  )
) AS j;
```

---

## 5.3 Cas XML – Fichier clients.xml

### 5.3.1 Créer le fichier XML

Créer le fichier **data/clients.xml** :

```bash
<clients>
  <client>
    <nom>alice</nom>
    <email>alice@example.com</email>
    <date_naissance>1990-05-10</date_naissance>
    <total_achats>1500.00</total_achats>
  </client>
  <client>
    <nom>bob</nom>
    <email>bob(at)example.com</email>
    <date_naissance>1985-03-21</date_naissance>
    <total_achats>200.00</total_achats>
  </client>
</clients>
```

### 5.3.2 Table staging XML

```sql
USE etl;

CREATE TABLE IF NOT EXISTS clients_xml_raw (
  xml_doc LONGTEXT
);
```

### 5.3.3 Charger le XML brut

```sql
INSERT INTO clients_xml_raw (xml_doc)
VALUES (LOAD_FILE('/path/vers/data/clients.xml'));
```

### 5.3.4 Extraction simple avec `ExtractValue`

```sql
USE target_db;

-- Exemple : extraire uniquement le premier client pour la démo
INSERT INTO clients_dest (nom_normalise, email_valide, age, segment)
SELECT
  CONCAT(
    UPPER(SUBSTRING(ExtractValue(xml_doc, '/clients/client[1]/nom'), 1, 1)),
    LOWER(SUBSTRING(ExtractValue(xml_doc, '/clients/client[1]/nom'), 2))
  ),
  ExtractValue(xml_doc, '/clients/client[1]/email'),
  0,           -- pour rester simple, on ne calcule pas l'âge ici
  'XML_demo'   -- segment symbolique pour la démo
FROM etl.clients_xml_raw;
```

> Pour un traitement complet multi-clients, on utiliserait une approche plus avancée (boucles ou pré-processing).

---

# 6. Automatisation : procédures + événements + scripts externes

Dernier volet : rendre l’ETL **récurrent**.

---

## 6.1 Procédure ETL complète basée sur CSV

On va créer une procédure qui :

1. Vide la table staging CSV.

2. Charge le fichier CSV.

3. Remplit la table cible.

4. Loggue l’exécution.

### 6.1.1 Table de log pour cette procédure

```sql
USE etl;

CREATE TABLE IF NOT EXISTS log_etl_csv (
  id INT AUTO_INCREMENT PRIMARY KEY,
  date_execution DATETIME,
  fichier VARCHAR(255),
  lignes_inserées INT
);
```

### 6.1.2 Procédure progressive

```sql
USE target_db;

DELIMITER $$

CREATE PROCEDURE etl_import_clients_from_csv(IN p_path VARCHAR(255))
BEGIN
  DECLARE v_rows INT;

  -- 1) Vider la table staging
  TRUNCATE TABLE etl.clients_csv_staging;

  -- 2) Charger le fichier CSV dans la table staging
  SET @sql_load = CONCAT(
    "LOAD DATA INFILE '", p_path, "' ",
    "INTO TABLE etl.clients_csv_staging ",
    "FIELDS TERMINATED BY ',' ENCLOSED BY '\"' ",
    "IGNORE 1 LINES"
  );
  PREPARE stmt_load FROM @sql_load;
  EXECUTE stmt_load;
  DEALLOCATE PREPARE stmt_load;

  -- 3) Transférer vers la table finale
  INSERT INTO clients_dest (nom_normalise, email_valide, age, segment)
  SELECT
    CONCAT(
      UPPER(SUBSTRING(nom, 1, 1)),
      LOWER(SUBSTRING(nom, 2))
    ),
    CASE 
      WHEN TRIM(email) REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'
      THEN TRIM(LOWER(email))
      ELSE NULL
    END,
    TIMESTAMPDIFF(YEAR, date_naissance, CURDATE()),
    CASE 
      WHEN total_achats > 10000 THEN 'Premium'
      WHEN total_achats > 1000 THEN 'Standard'
      ELSE 'Basic'
    END
  FROM etl.clients_csv_staging;

  SET v_rows = ROW_COUNT();

  -- 4) Log de l'exécution
  INSERT INTO etl.log_etl_csv (date_execution, fichier, lignes_inserées)
  VALUES (NOW(), p_path, v_rows);
END$$

DELIMITER ;
```

Exemple d’appel :

```sql
CALL etl_import_clients_from_csv('/path/vers/data/clients.csv');
```

---

## 6.2 Événement planifié MariaDB

On va planifier l’exécution automatique de cette procédure, par exemple tous les jours à 2h.

### 6.2.1 Activer le scheduler

```sql
SET GLOBAL event_scheduler = ON;
```

### 6.2.2 Créer l’événement

```sql
DELIMITER $$

CREATE EVENT IF NOT EXISTS evt_etl_clients_daily
ON SCHEDULE EVERY 1 DAY
STARTS '2025-01-01 02:00:00'
DO
  CALL etl_import_clients_from_csv('/path/vers/data/clients.csv');$$

DELIMITER ;
```

---

## 6.3 Script externe pour lancer l’ETL (Linux)

Créer le fichier **/usr/local/bin/run_etl_clients.sh** :

```bash
#!/usr/bin/env bash
# Script simple qui lance l'ETL clients depuis la ligne de commande

mysql -u etl_user -p'Mot2Passe!' -e \
  "CALL target_db.etl_import_clients_from_csv('/path/vers/data/clients.csv');"
```

Puis :

```bash
chmod +x /usr/local/bin/run_etl_clients.sh
```

Planification via `cron` :

```bash
0 3 * * * /usr/local/bin/run_etl_clients.sh
```

---

# 7. Comment exécuter les scripts : mysql-client et MySQL Workbench

Pour que l’étudiant soit à l’aise, on détaille **comment utiliser concrètement ces scripts**.

---

## 7.1 Avec `mysql` en ligne de commande

1. **Connexion à MariaDB** :
   
   ```bash
   mysql -h serveur -u etl_user -p
   ```

2. **Exécuter un script collé** :
   
   - Copier le bloc SQL dans le terminal après le prompt `MariaDB [(none)]>`
   
   - Valider avec `;` et Entrée.

3. **Exécuter un fichier `.sql`** :
   
   ```bash
   mysql -h serveur -u etl_user -p < mon_script.sql
   ```

4. **Vérifier le résultat** :
   
   ```sql
   USE target_db;
   SELECT * FROM clients_dest;
   SELECT * FROM etl.log_etl_csv;
   ```

---

## 7.2 Avec MySQL Workbench

1. **Ouvrir une connexion** à la base MariaDB.

2. Cliquer sur **“SQL +”** (nouvel onglet “Query”).

3. Coller un bloc SQL (par exemple, la procédure) dans l’éditeur.

4. Cliquer sur l’icône **“Éclair”** (Execute).

5. Pour voir les données :
   
   ```sql
   SELECT * FROM target_db.clients_dest;
   SELECT * FROM etl.log_import_clients;
   ```

6. Pour rejouer une procédure :
   
   ```sql
   CALL target_db.etl_import_clients();
   ```

---

# 8. Conclusion

En partant de **scripts très simples**, on a construit progressivement :

- un **transfert intra-instance** avec transformations,

- un **transfert inter-instances** via dump SQL et via CSV,

- des **ETL basés sur CSV, JSON, XML**,

- une **procédure ETL complète** encapsulant l’ETL CSV,

- une **automatisation** via événements et scripts externes.

Le tout repose exclusivement sur :

- **SQL natif + procédures stockées + événements MariaDB**,

- des fichiers **CSV/JSON/XML** très simples et fournis,

- un usage clair via **mysql-client** ou **MySQL Workbench**.

Si vous voulez, on peut maintenant ajouter **un set d’exercices pratiques** (mini-scenarii) qui réutilisent exactement ces scripts pour vérifier la compréhension et la maîtrise de vos participants.
