# Atelier - transfert des données avec python

## 1.Contexte & objectifs spécifiques Python

Avec Python + Pandas + SQLAlchemy, on cherche à :

- Lire des données MariaDB ou fichiers (CSV, JSON, XML).

- Les transformer avec Pandas (nettoyage, enrichissement).

- Les réinjecter en masse dans MariaDB.

- Pouvoir attaquer **une ou plusieurs instances MariaDB** simplement en changeant les URL de connexion.

**Quand préférer cette approche :**

- Règles métier un peu lourdes / compliquées à écrire en SQL.

- Croiser des données venant d’API, fichiers, autres systèmes.

- Produire des pipelines ETL réutilisables sous forme de scripts Python.

---

# 2. Préparation de l’environnement Python

## 2.1 Prérequis côté MariaDB

On garde l’idée :

- Serveur MariaDB avec les bases : `source_db`, `target_db`, `etl`.

- Un utilisateur `etl_user` avec les droits sur ces bases (déjà mis en place dans la première technique).

## 2.2 Créer un environnement Python

Sur votre machine (Linux ou Windows, terminal classique) :

```bash
python -m venv venv_etl
```

Activer l’environnement :

- Sous Linux / macOS :
  
  ```bash
  source venv_etl/bin/activate
  ```

- Sous Windows (PowerShell ou CMD) :
  
  ```bash
  venv_etl\Scripts\activate
  ```

Installer les bibliothèques nécessaires :

```bash
pip install pandas sqlalchemy mariadb
```

> On utilise le driver officiel MariaDB + dialecte SQLAlchemy `mariadb+mariadbconnector`. ([MariaDB](https://mariadb.com/resources/blog/using-sqlalchemy-with-mariadb-connector-python-part-1/?utm_source=chatgpt.com "Using SQLAlchemy with MariaDB Connector/Python: Part 1"))

---

# 3. Étape 1 – Script Python minimal : lire des données MariaDB avec Pandas

On commence par un script **très simple** : se connecter à MariaDB, lire une table avec Pandas, afficher les premières lignes.

Créer le fichier **etl/config.py** :

```python
# etl/config.py
# -----------------
# Centralise les URL de connexion à MariaDB pour éviter de les répéter.

from sqlalchemy import create_engine

# Connexion à l'instance principale MariaDB (source + cible sur le même serveur)
# Adapter user / mot de passe / host / port selon votre environnement.
MARIADB_USER = "etl_user"
MARIADB_PASSWORD = "Mot2Passe!"
MARIADB_HOST = "127.0.0.1"
MARIADB_PORT = 3306

# URL pour la base source_db
SOURCE_DB_URL = (
    f"mariadb+mariadbconnector://{MARIADB_USER}:{MARIADB_PASSWORD}"
    f"@{MARIADB_HOST}:{MARIADB_PORT}/source_db"
)

# URL pour la base target_db
TARGET_DB_URL = (
    f"mariadb+mariadbconnector://{MARIADB_USER}:{MARIADB_PASSWORD}"
    f"@{MARIADB_HOST}:{MARIADB_PORT}/target_db"
)

def get_source_engine():
    """Retourne un Engine SQLAlchemy connecté à la base source_db."""
    return create_engine(SOURCE_DB_URL)

def get_target_engine():
    """Retourne un Engine SQLAlchemy connecté à la base target_db."""
    return create_engine(TARGET_DB_URL)
```

Créer le fichier **etl/read_source_basic.py** :

```python
# etl/read_source_basic.py
# ------------------------
# Objectif : tester la connexion à MariaDB et lire la table source_db.clients_src
# avec Pandas, puis afficher quelques lignes.

import pandas as pd
from config import get_source_engine

def main():
    # Création de l'engine SQLAlchemy pour la base source_db
    engine = get_source_engine()

    # Lecture simple de la table clients_src avec Pandas
    df = pd.read_sql("SELECT * FROM clients_src", con=engine)

    # Affiche les 5 premières lignes dans le terminal
    print("Aperçu des données source :")
    print(df.head())

if __name__ == "__main__":
    main()
```

### Comment exécuter et vérifier

Dans le terminal, depuis le dossier où se trouve le package `etl` :

```bash
# Activer l'environnement si nécessaire
# source venv_etl/bin/activate  (Linux/macOS)
# venv_etl\Scripts\activate     (Windows)

python -m etl.read_source_basic
```

Vous devez voir dans la console un mini tableau avec les premières lignes de `clients_src` (colonnes `id`, `nom`, `email`, etc.).

---

# 4. Étape 2 – Exporter la source vers un CSV (petit ETL “E”)

On ajoute un script qui sort les données de `source_db.clients_src` vers un fichier CSV.

Créer le fichier **etl/export_clients_csv.py** :

```python
# etl/export_clients_csv.py
# -------------------------
# Objectif : extraire les clients depuis source_db.clients_src
# et les écrire dans un fichier CSV (première brique ETL : Extract).

import pandas as pd
from config import get_source_engine
from pathlib import Path

def main():
    engine = get_source_engine()

    # 1) Lire toutes les données de la table source
    query = "SELECT id, nom, email, date_naissance, total_achats, date_creation FROM clients_src"
    df = pd.read_sql(query, con=engine)

    # 2) Définir le dossier et le nom du fichier de sortie
    output_dir = Path("data")
    output_dir.mkdir(exist_ok=True)
    output_file = output_dir / "clients_export.csv"

    # 3) Écrire le DataFrame dans un CSV
    # index=False pour ne pas ajouter une colonne d'index inutile
    df.to_csv(output_file, index=False)

    print(f"Export terminé. Fichier généré : {output_file.resolve()}")

if __name__ == "__main__":
    main()
```

### Résultat attendu

- Un dossier `data/` est créé à la racine.

- Un fichier `data/clients_export.csv` apparaît, contenant les lignes de `clients_src`.

---

# 5. Étape 3 – Transformer avec Pandas, charger dans target_db (ETL complet simple)

Maintenant, on va :

1. Lire depuis `source_db` (ou le CSV).

2. Nettoyer / enrichir les données avec Pandas.

3. Charger le résultat dans `target_db.clients_dest`.

## 5.1 Version progressive : transformation dans Pandas

Créer le fichier **etl/etl_clients_basic.py** :

```python
# etl/etl_clients_basic.py
# ------------------------
# Objectif : ETL complet simple:
# - Extract : lire les clients depuis source_db.clients_src
# - Transform : nettoyer / enrichir via Pandas
# - Load : écrire le résultat dans target_db.clients_dest

import pandas as pd
from config import get_source_engine, get_target_engine

def extract_clients():
    """Lit les clients depuis la base source_db et retourne un DataFrame Pandas."""
    engine_source = get_source_engine()
    query = """
        SELECT id, nom, email, date_naissance, total_achats, date_creation
        FROM clients_src
    """
    return pd.read_sql(query, con=engine_source)

def transform_clients(df: pd.DataFrame) -> pd.DataFrame:
    """
    Applique une série de transformations :
    - normalisation du nom
    - nettoyage + validation de l'email
    - calcul de l'âge
    - segmentation en fonction du total_achats
    """

    df = df.copy()  # on évite de modifier le DataFrame original

    # Nom normalisé : première lettre en majuscule, le reste en minuscule
    df["nom_normalise"] = df["nom"].str.strip().str.lower().str.capitalize()

    # Nettoyage du champ email
    df["email_clean"] = df["email"].astype(str).str.strip().str.lower()

    # Validation simple des emails (regex basique)
    email_regex = r"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$"
    df["email_valide"] = df["email_clean"].where(df["email_clean"].str.match(email_regex), None)

    # Calcul approximatif de l'âge (en années)
    today = pd.Timestamp("today").normalize()
    df["age"] = (today - pd.to_datetime(df["date_naissance"])).dt.days // 365

    # Segmentation commerciale simple
    conditions = [
        df["total_achats"] > 10000,
        df["total_achats"].between(1000, 10000, inclusive="left"),
        df["total_achats"] < 1000,
    ]
    choix = ["Premium", "Standard", "Basic"]
    df["segment"] = pd.Series(pd.Categorical(pd.np.select(conditions, choix, default="Basic")))

    # On ne garde que les colonnes cibles
    result = df[["nom_normalise", "email_valide", "age", "segment"]].copy()
    return result

def load_clients(df_clean: pd.DataFrame):
    """
    Charge le DataFrame transformé dans la table target_db.clients_dest.
    Stratégie simple : on vide la table puis on recharge.
    """

    engine_target = get_target_engine()

    # On purge la table destination (pour une démo ; en production on serait plus fin)
    with engine_target.begin() as conn:
        conn.execute("TRUNCATE TABLE clients_dest")

    # to_sql va faire les insertions en "bulk" via SQLAlchemy
    df_clean.to_sql(
        "clients_dest",
        con=engine_target,
        if_exists="append",  # on insère après le TRUNCATE
        index=False,
    )

def main():
    print(">>> Extraction des clients source...")
    df_source = extract_clients()
    print(f"{len(df_source)} lignes extraites")

    print(">>> Transformation des données...")
    df_clean = transform_clients(df_source)
    print("Aperçu des données transformées :")
    print(df_clean.head())

    print(">>> Chargement vers target_db.clients_dest...")
    load_clients(df_clean)

    print("ETL terminé avec succès.")

if __name__ == "__main__":
    main()
```

> Note : `pd.np` est conservé ici pour garder le script simple ; vous pouvez remplacer par `import numpy as np` et `np.select` si vous préférez.

### Comment exécuter

```bash
python -m etl.etl_clients_basic
```

Ce que vous devez constater :

- Le script affiche le nombre de lignes extraites.

- Un aperçu des données transformées s’affiche (nom normalisé, email_valide, age, segment).

- Dans MariaDB (via `mysql` ou Workbench) :
  
  ```sql
  USE target_db;
  SELECT * FROM clients_dest;
  ```
  
  → Vous voyez les enregistrements nettoyés.

---

# 6. Étape 4 – Intégrer un CSV externe dans le même pipeline

On va réutiliser les mêmes transformations, mais **en partant d’un fichier CSV externe**, par exemple fourni par un autre système.

## 6.1 Fichier source CSV pour la démo

Créer le fichier **data/clients_externes.csv** :

```bash
id,nom,email,date_naissance,total_achats,date_creation
1001,alice,alice@example.com,1990-05-10,1500.00,2024-12-01 10:15:00
1002,bob,"bob(at)example.com",1985-03-21,200.00,2024-11-15 09:00:00
1003,CAROL,carol@example.com,1978-11-02,12000.00,2024-10-05 14:30:00
```

## 6.2 Script ETL basé sur CSV

Créer le fichier **etl/etl_clients_from_csv.py** :

```python
# etl/etl_clients_from_csv.py
# ---------------------------
# Objectif : démontrer un ETL basé sur un fichier CSV externe.
# - Lecture du CSV avec Pandas
# - Transformation (on réutilise transform_clients du script précédent)
# - Chargement dans target_db.clients_dest

import pandas as pd
from pathlib import Path
from config import get_target_engine
from etl_clients_basic import transform_clients, load_clients

def main():
    # 1) Localiser le fichier CSV
    csv_path = Path("data") / "clients_externes.csv"
    if not csv_path.exists():
        raise FileNotFoundError(f"Fichier introuvable : {csv_path.resolve()}")

    # 2) Lecture dans un DataFrame Pandas
    df = pd.read_csv(csv_path)

    # 3) Transformation via la fonction existante
    df_clean = transform_clients(df)

    # 4) Chargement dans la base cible
    load_clients(df_clean)

    print("ETL CSV -> MariaDB terminé.")

if __name__ == "__main__":
    main()
```

### Exécution

```bash
python -m etl.etl_clients_from_csv
```

Résultat : `target_db.clients_dest` contient maintenant les clients du CSV, transformés comme pour la base source.

---

# 7. Étape 5 – Ajouter JSON et XML au pipeline

On va montrer, de façon simple, comment :

- Lire un **JSON** (liste d’objets clients).

- Lire un **XML** (structure clients / client).

- Intégrer ensuite dans `clients_dest` avec la même logique de transformation.

## 7.1 Fichier JSON d’exemple

Créer le fichier **data/clients.json** :

```bash
[
  {
    "nom": "alice",
    "email": "alice@example.com",
    "date_naissance": "1990-05-10",
    "total_achats": 1500.0,
    "date_creation": "2024-12-01T10:15:00"
  },
  {
    "nom": "bob",
    "email": "bob(at)example.com",
    "date_naissance": "1985-03-21",
    "total_achats": 200.0,
    "date_creation": "2024-11-15T09:00:00"
  }
]
```

## 7.2 Fichier XML d’exemple

Créer le fichier **data/clients.xml** :

```bash
<clients>
  <client>
    <nom>alice</nom>
    <email>alice@example.com</email>
    <date_naissance>1990-05-10</date_naissance>
    <total_achats>1500.0</total_achats>
    <date_creation>2024-12-01 10:15:00</date_creation>
  </client>
  <client>
    <nom>bob</nom>
    <email>bob(at)example.com</email>
    <date_naissance>1985-03-21</date_naissance>
    <total_achats>200.0</total_achats>
    <date_creation>2024-11-15 09:00:00</date_creation>
  </client>
</clients>
```

## 7.3 Script Python pour JSON + XML → Pandas → MariaDB

Créer le fichier **etl/etl_clients_json_xml.py** :

```python
# etl/etl_clients_json_xml.py
# ---------------------------
# Objectif : montrer comment prendre en charge des fichiers JSON et XML
# pour alimenter MariaDB via Pandas + SQLAlchemy.

import pandas as pd
from pathlib import Path
from config import get_target_engine
from etl_clients_basic import transform_clients, load_clients

def load_from_json(json_path: Path) -> pd.DataFrame:
    """
    Charge un fichier JSON contenant une liste d'objets clients.
    Chaque objet doit contenir : nom, email, date_naissance, total_achats, date_creation.
    """
    df = pd.read_json(json_path)
    return df

def load_from_xml(xml_path: Path) -> pd.DataFrame:
    """
    Charge un fichier XML de la forme :
    <clients><client>...</client><client>...</client></clients>
    en DataFrame Pandas.
    """
    # pandas.read_xml nécessite lxml installé (pip install lxml)
    df = pd.read_xml(xml_path, xpath=".//client")
    return df

def main():
    json_path = Path("data") / "clients.json"
    xml_path = Path("data") / "clients.xml"

    if not json_path.exists():
        raise FileNotFoundError(f"JSON introuvable : {json_path.resolve()}")
    if not xml_path.exists():
        raise FileNotFoundError(f"XML introuvable : {xml_path.resolve()}")

    # 1) Charger depuis JSON
    df_json = load_from_json(json_path)
    df_json["source"] = "json"

    # 2) Charger depuis XML
    df_xml = load_from_xml(xml_path)
    df_xml["source"] = "xml"

    # 3) Fusionner les deux sources
    df_all = pd.concat([df_json, df_xml], ignore_index=True, sort=False)

    # 4) Transformer les données (normalisation, email, etc.)
    df_clean = transform_clients(df_all)

    # 5) Charger dans MariaDB
    load_clients(df_clean)

    print("ETL JSON + XML -> MariaDB terminé.")

if __name__ == "__main__":
    main()
```

> Remarque : pour `pd.read_xml`, installez `lxml` si nécessaire :  
> `pip install lxml`.

Exécution :

```bash
python -m etl.etl_clients_json_xml
```

Puis vérifiez dans MariaDB que `clients_dest` contient les données JSON + XML transformées.

---

# 8. Transfert entre deux instances MariaDB avec Python (inter-serveurs)

Pour gérer **deux instances MariaDB différentes**, il suffit de définir deux URL de connexion dans `config.py`.

Exemple :

- `source_db` sur `serveur_a`.

- `target_db` sur `serveur_b`.

Adapter **etl/config.py** :

```python
# etl/config.py (exemple inter-instances)
from sqlalchemy import create_engine

# Instance source (serveur A)
SRC_USER = "etl_user"
SRC_PASSWORD = "Mot2PasseSource!"
SRC_HOST = "10.0.0.10"   # adresse du serveur A
SRC_PORT = 3306

# Instance cible (serveur B)
TGT_USER = "etl_user"
TGT_PASSWORD = "Mot2PasseCible!"
TGT_HOST = "10.0.0.20"   # adresse du serveur B
TGT_PORT = 3306

SOURCE_DB_URL = (
    f"mariadb+mariadbconnector://{SRC_USER}:{SRC_PASSWORD}"
    f"@{SRC_HOST}:{SRC_PORT}/source_db"
)

TARGET_DB_URL = (
    f"mariadb+mariadbconnector://{TGT_USER}:{TGT_PASSWORD}"
    f"@{TGT_HOST}:{TGT_PORT}/target_db"
)

def get_source_engine():
    return create_engine(SOURCE_DB_URL)

def get_target_engine():
    return create_engine(TARGET_DB_URL)
```

Sans changer un seul caractère dans `etl_clients_basic.py`, votre script fera alors :

- `SELECT` sur `serveur_a/source_db.clients_src`.

- Transformations via Pandas.

- `TRUNCATE + INSERT` sur `serveur_b/target_db.clients_dest`.

C’est la même logique fonctionnelle que dans la technique SQL native, mais **pilotée par Python**.

---

# 9. Automatisation des scripts Python (cron, tâches planifiées)

## 9.1 Exemple Linux – cron

On suppose que :

- Le projet est dans `/opt/etl_mariadb/`.

- L’environnement virtuel est `/opt/etl_mariadb/venv_etl`.

Créer le fichier **/usr/local/bin/run_etl_clients_python.sh** :

```bash
#!/usr/bin/env bash
# run_etl_clients_python.sh
# -------------------------
# Active l'environnement virtuel et lance le script ETL Python.

cd /opt/etl_mariadb || exit 1

# Activation de l'environnement virtuel
source venv_etl/bin/activate

# Lancement du script ETL principal
python -m etl.etl_clients_basic
```

Puis :

```bash
sudo chmod +x /usr/local/bin/run_etl_clients_python.sh
```

Ajout dans `crontab` (exécution tous les jours à 02h00) :

```bash
0 2 * * * /usr/local/bin/run_etl_clients_python.sh >> /var/log/etl_clients_python.log 2>&1
```

## 9.2 Exemple Windows – Tâche planifiée

1. Ouvrir le **Planificateur de tâches**.

2. Créer une tâche qui exécute :
   
   - Programme : `C:\Path\To\Python\python.exe`
   
   - Arguments : `-m etl.etl_clients_basic`
   
   - Dossier de démarrage : dossier du projet (là où se trouve `etl/` et le venv).

3. Avant, vérifier que l’environnement virtuel est bien pris en compte (soit en appelant `python` du venv, soit en activant le venv dans un `.bat` intermédiaire).

---

# 10. Résumé des scripts et fichiers fournis

**Scripts principaux :**

- `etl/config.py`  
  → centralise les URL de connexion MariaDB.

- `etl/read_source_basic.py`  
  → test de connexion + lecture simple avec Pandas.

- `etl/export_clients_csv.py`  
  → export MariaDB → CSV.

- `etl/etl_clients_basic.py`  
  → ETL complet source_db → target_db (transformations dans Pandas).

- `etl/etl_clients_from_csv.py`  
  → ETL complet à partir d’un CSV externe.

- `etl/etl_clients_json_xml.py`  
  → ETL complet à partir de JSON + XML.

**Fichiers de données d’exemple :**

- `data/clients_export.csv` (généré).

- `data/clients_externes.csv` (fourni).

- `data/clients.json` (fourni).

- `data/clients.xml` (fourni).

**Automatisation :**

- `run_etl_clients_python.sh` (Linux, cron).

- Tâche planifiée Windows avec `python -m etl.etl_clients_basic`.

---

# Partie II - Configuration générique des connexions (multi-instances)

Créer le fichier **etl/config_generic.py** :

```python
# etl/config_generic.py
# ---------------------
# Ce module centralise la configuration des connexions aux différentes
# instances MySQL/MariaDB utilisées pour les transferts.
#
# L'idée :
#   - Déclarer plusieurs connexions nommées (ex : "local_source", "remote_prod")
#   - Utiliser ces noms dans les scripts (pas besoin de retaper l'URL complète)
#
# On utilise SQLAlchemy pour construire les "Engine" de connexion.

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict

from sqlalchemy import create_engine
from sqlalchemy.engine import Engine


@dataclass
class DbConfig:
    """Représente la configuration minimale d'une base MySQL/MariaDB."""

    name: str        # Nom logique (ex : "local_source")
    user: str        # Nom d'utilisateur SQL
    password: str    # Mot de passe SQL
    host: str        # Adresse du serveur (IP ou hostname)
    port: int        # Port du serveur (3306 par défaut)
    database: str    # Nom de la base par défaut


# Dictionnaire de connexions disponibles.
# À ADAPTER en fonction de ton environnement.
#
# Exemple :
#   - "local_source" : instance de développement sur 127.0.0.1
#   - "remote_target": instance distante (prod ou pré-prod)

CONNECTIONS: Dict[str, DbConfig] = {
    "local_source": DbConfig(
        name="local_source",
        user="etl_user",
        password="Mot2PasseSource!",
        host="127.0.0.1",
        port=3306,
        database="source_db",
    ),
    "remote_target": DbConfig(
        name="remote_target",
        user="etl_user",
        password="Mot2PasseCible!",
        host="192.168.1.50",  # exemple : autre instance MySQL/MariaDB
        port=3306,
        database="target_db",
    ),
    # Ajouter ici d'autres connexions (test, préprod, etc.)
}


def build_sqlalchemy_url(cfg: DbConfig) -> str:
    """
    Construit l'URL de connexion SQLAlchemy pour MariaDB/MySQL.

    On utilise le dialecte "mariadb+mariadbconnector", qui fonctionne
    généralement bien avec MariaDB et MySQL.
    """
    return (
        f"mariadb+mariadbconnector://{cfg.user}:{cfg.password}"
        f"@{cfg.host}:{cfg.port}/{cfg.database}"
    )


def get_engine(name: str) -> Engine:
    """
    Retourne un Engine SQLAlchemy pour la connexion nommée.

    :param name: Nom logique de la connexion (clé dans CONNECTIONS)
    :raises KeyError: si le nom est inconnu
    """
    cfg = CONNECTIONS[name]
    url = build_sqlalchemy_url(cfg)
    return create_engine(url, future=True)
```

---

## 2. Point d’extension pour les transformations

Créer le fichier **etl/transform.py** :

```python
# etl/transform.py
# ----------------
# Ce module contient une fonction de transformation générique de DataFrame.
#
# Objectif :
#   - Centraliser les règles métier (nettoyage, mappings, etc.)
#   - Laisser la possibilité au formateur ou à l'étudiant de modifier
#     ce comportement sans toucher au cœur du moteur de transfert.
#
# Par défaut, la transformation est neutre : on renvoie le DataFrame tel quel.

from __future__ import annotations

import pandas as pd


def transform_df(df: pd.DataFrame) -> pd.DataFrame:
    """
    Transforme un DataFrame avant son chargement dans la base cible.

    Version par défaut :
    - ne fait rien, renvoie df tel quel.
    - tu peux enrichir cette fonction avec des règles métier :
      normalisation, filtrage, calcul de colonnes, etc.

    :param df: DataFrame source (après lecture depuis la DB ou un fichier)
    :return: DataFrame transformé (prêt à être chargé)
    """
    # Exemple (à adapter si besoin) :
    # if "nom" in df.columns:
    #     df["nom"] = (
    #         df["nom"]
    #         .astype(str)
    #         .str.strip()
    #         .str.lower()
    #         .str.capitalize()
    #     )

    return df
```

---

## 3. Migrateur interactif générique

Créer le fichier **etl/interactive_migrator.py** :

```python
# etl/interactive_migrator.py
# ---------------------------
# Script principal : propose un menu interactif pour effectuer
# différents scénarios de transfert :
#
#   1) Instance MySQL/MariaDB -> Instance MySQL/MariaDB (cas principal)
#   2) Fichier (CSV/JSON/Excel) -> MariaDB
#   3) MariaDB -> Fichier (CSV/JSON/Excel)
#
# L'utilisateur saisit :
#   - les connexions (source, destination)
#   - la requête SQL source
#   - le nom de la table destination
#   - les chemins de fichiers
#
# Le code s'appuie sur :
#   - config_generic.get_engine pour les connexions
#   - pandas pour les DataFrames
#   - transform.transform_df pour appliquer des règles métier.

from __future__ import annotations

import sys
from pathlib import Path
from typing import Optional

import pandas as pd

from config_generic import CONNECTIONS, get_engine
from transform import transform_df


# --------------------
# Utilitaires généraux
# --------------------


def choose_connection(prompt: str) -> str:
    """
    Demande à l'utilisateur de choisir une connexion parmi celles déclarées.

    :param prompt: Message à afficher à l'utilisateur
    :return: Nom logique de la connexion choisie (clé dans CONNECTIONS)
    """
    print(prompt)
    print("Connexions disponibles :")
    for name, cfg in CONNECTIONS.items():
        print(f"  - {name} ({cfg.user}@{cfg.host}/{cfg.database})")

    while True:
        choice = input("Nom de la connexion : ").strip()
        if choice in CONNECTIONS:
            return choice
        print("Nom inconnu, merci de réessayer.")


def ask_non_empty(prompt: str) -> str:
    """
    Demande à l'utilisateur une chaîne non vide.
    """
    while True:
        value = input(prompt).strip()
        if value:
            return value
        print("La valeur ne peut pas être vide.")


def ask_path(prompt: str) -> Path:
    """
    Demande un chemin de fichier à l'utilisateur.
    """
    while True:
        value = input(prompt).strip()
        if value:
            return Path(value)


def ask_yes_no(prompt: str, default: bool = False) -> bool:
    """
    Demande une réponse oui/non à l'utilisateur.

    :param prompt: Message (sans suffixe)
    :param default: Valeur par défaut si l'utilisateur appuie juste sur Entrée
    """
    suffix = "[O/n]" if default else "[o/N]"
    while True:
        value = input(f"{prompt} {suffix} ").strip().lower()
        if not value:
            return default
        if value in ("o", "oui", "y", "yes"):
            return True
        if value in ("n", "non", "no"):
            return False
        print("Réponse non comprise, merci de répondre par o/n.")


# -----------------------------
# 1) Transfert DB -> DB (principal)
# -----------------------------


def db_to_db_transfer():
    """
    Transfert générique entre deux instances MySQL/MariaDB.

    Étapes :
      1) Choix connexion source
      2) Saisie de la requête SQL source
      3) Choix connexion destination
      4) Saisie du nom de la table destination
      5) Optionnel : TRUNCATE de la table destination
      6) Optionnel : transformation via transform_df
      7) Chargement en masse via df.to_sql
    """
    print("\n--- Transfert DB -> DB (MySQL/MariaDB) ---")

    # 1) Choix de la connexion source
    src_name = choose_connection("Sélectionnez la connexion SOURCE :")
    src_engine = get_engine(src_name)

    # 2) Saisie de la requête SQL
    print("\nSaisissez la requête SQL source (terminer par une ligne vide) :")
    print("Exemple : SELECT * FROM clients_src")
    print("(Astuce : vous pouvez copier/coller une requête complète ici.)")

    lines = []
    while True:
        line = input()
        if not line.strip():
            break
        lines.append(line)
    query = "\n".join(lines).strip()

    if not query:
        print("Aucune requête fournie, abandon.")
        return

    # 3) Exécution de la requête et chargement dans un DataFrame
    print("Exécution de la requête sur la base source...")
    df = pd.read_sql_query(query, con=src_engine)
    print(f"{len(df)} lignes lues depuis la source.")

    if len(df) == 0:
        print("Aucune ligne à transférer, fin.")
        return

    # 4) Choix de la connexion destination
    dst_name = choose_connection("\nSélectionnez la connexion DESTINATION :")
    dst_engine = get_engine(dst_name)

    # 5) Nom de la table destination
    dest_table = ask_non_empty(
        "Nom de la table destination (ex : clients_dest) : "
    )

    # 6) Option : TRUNCATE ?
    if ask_yes_no(f"Vider la table '{dest_table}' avant chargement ?", default=False):
        print(f"TRUNCATE TABLE {dest_table}...")
        with dst_engine.begin() as conn:
            conn.execute(f"TRUNCATE TABLE {dest_table}")

    # 7) Option : prévisualiser les données brutes
    if ask_yes_no("Afficher un aperçu des données brutes (5 lignes) ?", default=True):
        print(df.head())

    # 8) Option : transformation
    if ask_yes_no("Appliquer transform_df(df) avant chargement ?", default=True):
        df = transform_df(df)
        print("Transformations appliquées.")
        if ask_yes_no("Afficher un aperçu des données transformées ?", default=False):
            print(df.head())

    # 9) Chargement dans la base destination
    print(f"Chargement vers {dst_name}.{dest_table} ...")
    df.to_sql(
        dest_table,
        con=dst_engine,
        if_exists="append",  # append : on ajoute; TRUNCATE optionnel géré plus haut
        index=False,
    )
    print("Transfert DB -> DB terminé.")


# ------------------------------------
# 2) Transfert Fichier -> MariaDB
# ------------------------------------


def load_file_to_db():
    """
    Transfert générique FICHIER -> MariaDB.

    Formats supportés :
      - CSV
      - JSON
      - Excel (xlsx)
    """
    print("\n--- Transfert Fichier -> MariaDB ---")

    # Choix format
    print("Format du fichier source :")
    print("  1) CSV")
    print("  2) JSON")
    print("  3) Excel (xlsx)")
    choice = ask_non_empty("Votre choix (1/2/3) : ")

    # Chemin du fichier
    path = ask_path("Chemin complet du fichier source : ")
    if not path.exists():
        print(f"Fichier introuvable : {path}")
        return

    # Lecture du fichier dans un DataFrame
    if choice == "1":
        df = pd.read_csv(path)
    elif choice == "2":
        df = pd.read_json(path)
    elif choice == "3":
        df = pd.read_excel(path)
    else:
        print("Choix de format invalide.")
        return

    print(f"{len(df)} lignes lues depuis le fichier.")

    if ask_yes_no("Afficher un aperçu des données (5 lignes) ?", default=True):
        print(df.head())

    # Connexion destination
    dst_name = choose_connection("Sélectionnez la connexion DESTINATION :")
    dst_engine = get_engine(dst_name)

    # Table destination
    dest_table = ask_non_empty(
        "Nom de la table destination (ex : clients_import) : "
    )

    # Option TRUNCATE
    if ask_yes_no(f"Vider la table '{dest_table}' avant chargement ?", default=False):
        with dst_engine.begin() as conn:
            conn.execute(f"TRUNCATE TABLE {dest_table}")

    # Transformation optionnelle
    if ask_yes_no("Appliquer transform_df(df) avant chargement ?", default=True):
        df = transform_df(df)
        print("Transformations appliquées.")

    # Chargement
    df.to_sql(
        dest_table,
        con=dst_engine,
        if_exists="append",
        index=False,
    )

    print("Transfert Fichier -> MariaDB terminé.")


# ------------------------------------
# 3) Transfert MariaDB -> Fichier
# ------------------------------------


def db_to_file():
    """
    Transfert générique MariaDB -> FICHIER.

    L'utilisateur fournit :
      - la connexion
      - la requête SQL
      - le format de sortie (CSV / JSON / Excel)
      - le chemin du fichier de sortie
    """
    print("\n--- Transfert MariaDB -> Fichier ---")

    # Connexion
    src_name = choose_connection("Sélectionnez la connexion SOURCE :")
    src_engine = get_engine(src_name)

    # Requête SQL
    print("\nSaisissez la requête SQL (terminer par une ligne vide) :")
    lines: list[str] = []
    while True:
        line = input()
        if not line.strip():
            break
        lines.append(line)
    query = "\n".join(lines).strip()
    if not query:
        print("Aucune requête fournie, abandon.")
        return

    # Exécution et DataFrame
    df = pd.read_sql_query(query, con=src_engine)
    print(f"{len(df)} lignes lues depuis la base.")

    if len(df) == 0:
        print("Aucune ligne à exporter, fin.")
        return

    if ask_yes_no("Afficher un aperçu des données (5 lignes) ?", default=True):
        print(df.head())

    # Choix du format de sortie
    print("Format de sortie :")
    print("  1) CSV")
    print("  2) JSON")
    print("  3) Excel (xlsx)")
    fmt = ask_non_empty("Votre choix (1/2/3) : ")

    # Chemin de sortie
    out_path = ask_path("Chemin du fichier de sortie (sera écrasé si existe) : ")

    # Export selon le format
    if fmt == "1":
        df.to_csv(out_path, index=False)
    elif fmt == "2":
        df.to_json(out_path, orient="records", force_ascii=False)
    elif fmt == "3":
        df.to_excel(out_path, index=False)
    else:
        print("Format invalide.")
        return

    print(f"Export terminé. Fichier généré : {out_path.resolve()}")


# -------------------
# Menu principal
# -------------------


def main() -> int:
    """
    Point d'entrée principal du script interactif.
    Affiche un menu et redirige vers les fonctions correspondantes.
    """
    while True:
        print("\n======================")
        print("  MENU MIGRATION ETL  ")
        print("======================")
        print("1) DB -> DB (MySQL/MariaDB)")
        print("2) Fichier -> MariaDB")
        print("3) MariaDB -> Fichier")
        print("4) Quitter")

        choice = input("Votre choix : ").strip()

        if choice == "1":
            db_to_db_transfer()
        elif choice == "2":
            load_file_to_db()
        elif choice == "3":
            db_to_file()
        elif choice == "4":
            print("Au revoir.")
            return 0
        else:
            print("Choix invalide, merci de réessayer.")


if __name__ == "__main__":
    sys.exit(main())
```

---

## 4. Comment utiliser concrètement ces scripts

### 4.1 Installation des dépendances (rappel)

Dans ton environnement Python (par exemple `venv_etl`) :

```bash
pip install pandas sqlalchemy mariadb lxml openpyxl
```

- `pandas` : manipulation de tables (DataFrame).

- `sqlalchemy` : abstraction de connexion SQL.

- `mariadb` : driver MariaDB/MySQL.

- `lxml` : utile pour certaines fonctions XML.

- `openpyxl` : écriture/lecture Excel.

Assure-toi que ton package `etl/` contient :

- `__init__.py` (fichier vide pour marquer le package)

- `config_generic.py`

- `transform.py`

- `interactive_migrator.py`

### 4.2 Lancer le migrateur interactif

Depuis la racine du projet (là où se trouve le dossier `etl/`) :

```bash
python -m etl.interactive_migrator
```

Tu verras le menu :

```text
======================
  MENU MIGRATION ETL
======================
1) DB -> DB (MySQL/MariaDB)
2) Fichier -> MariaDB
3) MariaDB -> Fichier
4) Quitter
```

---

### 4.3 Exemple concret : transfert entre deux instances MySQL/MariaDB

**Objectif** :  
copier des données de `serveur A` (table `clients_src`) vers `serveur B` (table `clients_dest`), avec possibilité de nettoyage via `transform_df`.

1. Vérifie que `config_generic.CONNECTIONS` contient bien :
   
   - `"local_source"` pointant vers `serveur A / source_db`
   
   - `"remote_target"` pointant vers `serveur B / target_db`

2. Menu : choisir `1) DB -> DB`.

3. Choisir la connexion SOURCE : `local_source`.

4. Coller la requête :
   
   ```sql
   SELECT id, nom, email, date_naissance, total_achats, date_creation
   FROM clients_src
   ```
   
   Puis entrer une ligne vide pour terminer.

5. Choisir la connexion DESTINATION : `remote_target`.

6. Saisir le nom de la table destination : `clients_dest`.

7. Choisir si tu veux faire `TRUNCATE` de `clients_dest` (oui/non).

8. Choisir si tu veux appliquer `transform_df` (si tu l’as enrichi, c’est ici que ça prend effet).

9. Le script charge en masse vers la destination.

---

### 4.4 Exemple : CSV → MariaDB

1. Prépare un fichier `data/clients.csv` avec la structure voulue.

2. Menu : choisir `2) Fichier -> MariaDB`.

3. Choisir `1) CSV`.

4. Donner le chemin complet du CSV (ex : `C:\Data\clients.csv` ou `/home/user/data/clients.csv`).

5. Choisir la connexion destination (ex : `remote_target`).

6. Saisir la table destination (ex : `clients_import`).

7. Choisir si tu veux vider la table avant.

8. Eventuellement appliquer `transform_df`.

9. Le DataFrame est inséré en bulk dans la table cible.

---

### 4.5 Exemple : MariaDB → JSON (export générique)

1. Menu : `3) MariaDB -> Fichier`.

2. Choisir la connexion source (ex : `remote_target`).

3. Coller une requête :
   
   ```sql
   SELECT nom, email, total_achats FROM clients_dest WHERE segment = 'Premium'
   ```

4. Choisir le format : `2) JSON`.

5. Donner un chemin (ex : `exports/premium_clients.json`).

6. Le fichier JSON est écrit avec l’ensemble des lignes retournées par la requête.

---

## 5. Comment rendre le code plus “intelligent” pour tes étudiants

Le point central est la fonction :

```python
# dans transform.py
def transform_df(df: pd.DataFrame) -> pd.DataFrame:
    ...
```

Tu peux :

- Y mettre **exactement les transformations** que tu avais en SQL natif (normalisation, segmentation, etc.).

- Montrer comment **commenter / décommenter** des portions de code pour activer ou désactiver certaines règles.

- Construire des **exercices** où les apprenants doivent :
  
  - ajouter une nouvelle colonne,
  
  - filtrer les lignes invalides,
  
  - gérer des cas comme `email` manquant, etc.

Le moteur générique (`interactive_migrator.py`) reste stable,  
et tu joues sur `transform_df` et `config_generic.CONNECTIONS` pour adapter les cas d’usage (multi-instances, transferts fichiers, etc.).


